{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from typing import List, Callable, Tuple ,Set , Dict\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "print(1)\n",
    "methods_list={}"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7902b2d656c89a8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate sample data\n",
    "num_entries = 1000\n",
    "num_queries = 10\n",
    "dims = [4, 8, 12]  # Example dimensions for different embedding types\n",
    "\n",
    "# Create sample database entries\n",
    "database = [\n",
    "    [np.random.random((dims[i],)).astype('float32') for i in range(len(dims))]\n",
    "    for _ in range(num_entries)\n",
    "]\n",
    "\n",
    "# Create sample queries\n",
    "queries = [\n",
    "    [np.random.random((dims[i],)).astype('float32') for i in range(len(dims))]\n",
    "    for _ in range(num_queries)\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59ef753365b7a4a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Type aliases for clarity\n",
    "EmbeddingsList = List[List[np.ndarray]]  # List of lists of embeddings\n",
    "SearchFunction = Callable[[List[np.ndarray]], Tuple[np.ndarray, np.ndarray]]\n",
    "\n",
    "def simple_concatenation_ann(data: EmbeddingsList) -> SearchFunction:\n",
    "    \"\"\"\n",
    "    Simple concatenation method. Concatenates embeddings as-is.\n",
    "\n",
    "    Args:\n",
    "        data: List where each entry is a list of embeddings (one per embedding type)\n",
    "\n",
    "    Returns:\n",
    "        search_function: Function that takes query embeddings and returns distances and indices\n",
    "    \"\"\"\n",
    "    # Validate and process data\n",
    "    if not data or not all(data):\n",
    "        raise ValueError(\"Data must be non-empty and all entries must have embeddings\")\n",
    "\n",
    "    # Get dimensions and validate consistency\n",
    "    num_embedding_types = len(data[0])\n",
    "    dims = [data[0][i].shape[-1] for i in range(num_embedding_types)]\n",
    "    total_dim = sum(dims)\n",
    "\n",
    "    # Concatenate all data\n",
    "    concatenated_data = np.hstack([\n",
    "        np.vstack([entry[i] for entry in data])\n",
    "        for i in range(num_embedding_types)\n",
    "    ]).astype('float32')\n",
    "\n",
    "    # Create and train index\n",
    "    index = faiss.IndexHNSWFlat(total_dim, 16)\n",
    "    index.hnsw.efConstruction = 100\n",
    "    index.hnsw.efSearch = 64\n",
    "    index.add(concatenated_data)\n",
    "\n",
    "    def search(query_embeddings: List[np.ndarray], k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if len(query_embeddings) != num_embedding_types:\n",
    "            raise ValueError(f\"Expected {num_embedding_types} embedding types, got {len(query_embeddings)}\")\n",
    "\n",
    "        concatenated_query = np.hstack(query_embeddings).astype('float32').reshape((1,-1))\n",
    "        distances, indices = index.search(concatenated_query, k)\n",
    "        return distances, indices\n",
    "\n",
    "    return search\n",
    "\n",
    "methods_list[\"Simple Concatenation\"] =simple_concatenation_ann"
   ],
   "id": "9c1a8012dc7fcf24",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Type aliases\n",
    "EmbeddingsList = List[List[np.ndarray]]\n",
    "SearchFunction = Callable[[List[np.ndarray]], Tuple[np.ndarray, np.ndarray]]\n",
    "\n",
    "def multi_index_ann(data: EmbeddingsList) -> SearchFunction:\n",
    "    \"\"\"\n",
    "    Creates separate indices for each embedding type and combines their results,\n",
    "    calculating distances to all embeddings for each candidate.\n",
    "    \n",
    "    Args:\n",
    "        data: List where each entry is a list of embeddings (one per embedding type)\n",
    "    \n",
    "    Returns:\n",
    "        search_function: Function that takes query embeddings and returns combined results\n",
    "    \"\"\"\n",
    "    if not data or not all(data):\n",
    "        raise ValueError(\"Data must be non-empty and all entries must have embeddings\")\n",
    "    \n",
    "    num_embedding_types = len(data[0])\n",
    "    entry_count = len(data)\n",
    "    dims = [data[0][i].shape[-1] for i in range(num_embedding_types)]\n",
    "    \n",
    "    # Create separate indices and store embeddings for each type\n",
    "    indices = []\n",
    "    embeddings_by_type = []\n",
    "    \n",
    "    for i in range(num_embedding_types):\n",
    "        embeddings = np.vstack([entry[i] for entry in data]).astype('float32')\n",
    "        embeddings_by_type.append(embeddings)\n",
    "        \n",
    "        index = faiss.IndexHNSWFlat(dims[i], 16)\n",
    "        index.hnsw.efConstruction = 100\n",
    "        index.hnsw.efSearch = 64\n",
    "        index.add(embeddings)\n",
    "        indices.append(index)\n",
    "    \n",
    "    def search(query_embeddings: List[np.ndarray], k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if len(query_embeddings) != num_embedding_types:\n",
    "            raise ValueError(f\"Expected {num_embedding_types} embedding types, got {len(query_embeddings)}\")\n",
    "        \n",
    "        # Search in each index separately to get initial candidate set\n",
    "        all_candidates: Set[int] = set()\n",
    "        \n",
    "        for i, (query, index) in enumerate(zip(query_embeddings, indices)):\n",
    "            query_array = query.reshape(1, -1).astype('float32')\n",
    "            _, indices_found = index.search(query_array, k)\n",
    "            all_candidates.update(indices_found[0])\n",
    "        \n",
    "        # Calculate distances for all candidates across all embedding types\n",
    "        candidate_scores = []\n",
    "        \n",
    "        for idx in all_candidates:\n",
    "            total_distance = 0\n",
    "            for i, query in enumerate(query_embeddings):\n",
    "                # Reshape for broadcasting\n",
    "                query_vector = query.reshape(1, -1)\n",
    "                db_vector = embeddings_by_type[i][idx].reshape(1, -1)\n",
    "                \n",
    "                # Calculate L2 distance\n",
    "                distance = np.linalg.norm(query_vector - db_vector)\n",
    "                total_distance += distance\n",
    "            \n",
    "            candidate_scores.append((idx, total_distance))\n",
    "        \n",
    "        # Sort by total distance and get top k\n",
    "        candidate_scores.sort(key=lambda x: x[1])\n",
    "        final_indices = [idx for idx, _ in candidate_scores[:k]]\n",
    "        final_distances = [dist for _, dist in candidate_scores[:k]]\n",
    "        \n",
    "        return (np.array(final_distances).reshape(1, -1), \n",
    "                np.array(final_indices).reshape(1, -1))\n",
    "    \n",
    "    return search\n",
    "\n",
    "\n",
    "methods_list[\"separate indexing\"] =multi_index_ann"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec3df9faf40bf514",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def normalized_scaled_ann(data: EmbeddingsList) -> SearchFunction:\n",
    "    \"\"\"\n",
    "    Normalized and scaled concatenation method. Normalizes each embedding type\n",
    "    and scales based on dimension size.\n",
    "    \n",
    "    Args:\n",
    "        data: List where each entry is a list of embeddings (one per embedding type)\n",
    "    \n",
    "    Returns:\n",
    "        search_function: Function that takes query embeddings and returns distances and indices\n",
    "    \"\"\"\n",
    "    if not data or not all(data):\n",
    "        raise ValueError(\"Data must be non-empty and all entries must have embeddings\")\n",
    "    \n",
    "    \n",
    "    num_embedding_types = len(data[0])\n",
    "    dims = [data[0][i].shape[-1] for i in range(num_embedding_types)]\n",
    "    total_dim = sum(dims)\n",
    "    \n",
    "    # Calculate scaling factors based on dimensions\n",
    "    max_dim = max(dims)\n",
    "    scaling_factors = [np.sqrt(max_dim / dim) for dim in dims]\n",
    "    \n",
    "    # Normalize and scale each embedding type, then concatenate\n",
    "    #do Z-score normalization.\n",
    "    # Find the maximum length of the embeddings in each column\n",
    "    numberOfEmbeddings =len(data[0])\n",
    "    max_lengths = [len(data[0][i]) for i in range(numberOfEmbeddings)]\n",
    "    \n",
    "    # Initialize lists to store sums and counts for each column\n",
    "    sums = [np.zeros(mlengths) for mlengths in max_lengths]\n",
    "    counts = len(data)\n",
    "    \n",
    "    # First pass: Calculate sums and counts for each index of embeddings in each column\n",
    "    for row in data:\n",
    "        for i in range(numberOfEmbeddings):\n",
    "            sums[i]+=row[i]\n",
    "    \n",
    "    \n",
    "    # Calculate means for each index of embeddings in each column\n",
    "    means = [su /counts for su in sums]\n",
    "    \n",
    "    # Initialize lists to store squared differences for each column\n",
    "    squared_diffs = [np.zeros(mlengths) for mlengths in max_lengths]\n",
    "    \n",
    "    # Second pass: Calculate squared differences from the mean for each index of embeddings in each column\n",
    "    for row in data:\n",
    "        for i in range(numberOfEmbeddings):\n",
    "            squared_diffs[i]+= np.power(row[i]- means[i],2)\n",
    "    \n",
    "    # Calculate standard deviations for each index of embeddings in each column\n",
    "    stds = [np.sqrt(squared_diff / counts) for squared_diff in squared_diffs]\n",
    "    \n",
    "    # to avoid division by zero\n",
    "    for std in stds:\n",
    "        std+= (std ==0 ) # mask is 1 where std == 0 \n",
    "    # Normalize the data\n",
    "    normalized_data = []\n",
    "    for row in data:\n",
    "        normalized_row = []\n",
    "        for mean ,std,ro in zip(means ,stds,row ):\n",
    "            normalized_row.append((ro-mean)/std)\n",
    "        normalized_data.append(normalized_row)\n",
    "\n",
    "    concatenated_data = np.hstack([\n",
    "        np.vstack([entry[i] for entry in normalized_data]) * scaling_factors[i]\n",
    "        for i in range(num_embedding_types)\n",
    "    ]).astype('float32')\n",
    "    \n",
    "    # Create and train index\n",
    "    index = faiss.IndexHNSWFlat(total_dim, 16)\n",
    "    index.hnsw.efConstruction = 100\n",
    "    index.hnsw.efSearch = 64\n",
    "    index.add(concatenated_data)\n",
    "    \n",
    "    def search(query_embeddings: List[np.ndarray], k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if len(query_embeddings) != num_embedding_types:\n",
    "            raise ValueError(f\"Expected {num_embedding_types} embedding types, got {len(query_embeddings)}\")\n",
    "        \n",
    "        # Apply the same normalization and scaling to query embeddings\n",
    "        normalized_scaled_query = np.hstack([\n",
    "            ((query_embeddings[i]-means[i])/stds[i]) * scaling_factors[i]\n",
    "            for i in range(num_embedding_types)\n",
    "        ]).astype('float32').reshape(1,-1)\n",
    "        \n",
    "        distances, indices = index.search(normalized_scaled_query, k)\n",
    "        #todo efficiently find real distances before normalizing\n",
    "        distances = [ ]\n",
    "        unnormalized_query =np.hstack([i for i in query_embeddings])\n",
    "        for idx in indices[0]:\n",
    "            unnormalized_vector =np.hstack([i for i in data[idx]])\n",
    "            distances.append(np.linalg.norm(unnormalized_vector-unnormalized_query))\n",
    "        distances = np.array([distances]).reshape((1,-1))\n",
    "        return distances, indices\n",
    "    \n",
    "    return search\n",
    "\n",
    "methods_list[\"Normalized and Scaled\"] =normalized_scaled_ann"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a593481582655df",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class RobustSearchConfig:\n",
    "    min_embeddings_required: int = 1  # Minimum embeddings needed for a match\n",
    "    use_rank_fusion: bool = True      # Whether to use rank-based fusion\n",
    "    outlier_threshold: float = 2.0    # Z-score threshold for outlier detection\n",
    "    rank_weights: List[float] = None  # Weights for different embedding types\n",
    "    \n",
    "def robust_multi_index_ann(data: EmbeddingsList) -> SearchFunction:\n",
    "    \"\"\"\n",
    "    Creates a robust ANN search that can handle unreliable embeddings.\n",
    "    \n",
    "    Args:\n",
    "        data: List where each entry is a list of embeddings (one per embedding type)\n",
    "    \n",
    "    Returns:\n",
    "        search_function: Function that takes query embeddings and returns robust results\n",
    "    \"\"\"\n",
    "    if not data or not all(data):\n",
    "        raise ValueError(\"Data must be non-empty and all entries must have embeddings\")\n",
    "    \n",
    "    num_embedding_types = len(data[0])\n",
    "    entry_count = len(data)\n",
    "    dims = [data[0][i].shape[-1] for i in range(num_embedding_types)]\n",
    "    \n",
    "    # Create indices and store embeddings\n",
    "    indices = []\n",
    "    embeddings_by_type = []\n",
    "    \n",
    "    # Calculate mean and std for each embedding type for normalization\n",
    "    means = []\n",
    "    stds = []\n",
    "    \n",
    "    for i in range(num_embedding_types):\n",
    "        embeddings = np.vstack([entry[i] for entry in data]).astype('float32')\n",
    "        embeddings_by_type.append(embeddings)\n",
    "        \n",
    "        # Store mean and std for normalization\n",
    "        means.append(np.mean(embeddings, axis=0))\n",
    "        stds.append(np.std(embeddings, axis=0))\n",
    "        \n",
    "        index = faiss.IndexHNSWFlat(dims[i], 16)\n",
    "        index.hnsw.efConstruction = 100\n",
    "        index.hnsw.efSearch = 64\n",
    "        index.add(embeddings)\n",
    "        indices.append(index)\n",
    "    \n",
    "    def reciprocal_rank_fusion(rankings: List[List[int]], k: int = 60) -> Dict[int, float]:\n",
    "        \"\"\"Combine multiple rankings using reciprocal rank fusion\"\"\"\n",
    "        fused_scores = {}\n",
    "        for rank_list in rankings:\n",
    "            for rank, idx in enumerate(rank_list):\n",
    "                if idx not in fused_scores:\n",
    "                    fused_scores[idx] = 0\n",
    "                fused_scores[idx] += 1.0 / (rank + k)\n",
    "        return fused_scores\n",
    "    \n",
    "    def calculate_normalized_distances(query: np.ndarray, embeddings: np.ndarray, \n",
    "                                      mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate normalized distances accounting for distribution\"\"\"\n",
    "        normalized_query = (query - mean) / std\n",
    "        normalized_embeddings = (embeddings - mean) / std\n",
    "        return np.linalg.norm(normalized_embeddings - normalized_query, axis=1)\n",
    "    \n",
    "    def search(query_embeddings: List[np.ndarray], k: int = 5, \n",
    "               config: RobustSearchConfig = RobustSearchConfig()) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if len(query_embeddings) != num_embedding_types:\n",
    "            raise ValueError(f\"Expected {num_embedding_types} embedding types, got {len(query_embeddings)}\")\n",
    "        \n",
    "        # Initialize storage for rankings and distances\n",
    "        all_rankings: List[List[int]] = []\n",
    "        all_distances: List[Dict[int, float]] = []\n",
    "        all_candidates: Set[int] = set()\n",
    "        \n",
    "        # Search in each index and store results\n",
    "        for i, (query, index) in enumerate(zip(query_embeddings, indices)):\n",
    "            query_array = query.reshape(1, -1).astype('float32')\n",
    "            distances, indices_found = index.search(query_array, k)\n",
    "            \n",
    "            # Store rankings and distances\n",
    "            all_rankings.append(indices_found[0].tolist())\n",
    "            all_candidates.update(indices_found[0])\n",
    "            \n",
    "            # Calculate normalized distances for all candidates\n",
    "            normalized_distances = calculate_normalized_distances(\n",
    "                query, embeddings_by_type[i], means[i], stds[i])\n",
    "            all_distances.append({idx: dist for idx, dist in enumerate(normalized_distances)})\n",
    "        \n",
    "        # Combine results using various robust methods\n",
    "        candidate_scores = []\n",
    "        \n",
    "        if config.use_rank_fusion:\n",
    "            # Use rank fusion for initial scoring\n",
    "            fused_scores = reciprocal_rank_fusion(all_rankings)\n",
    "            initial_candidates = set(fused_scores.keys())\n",
    "        else:\n",
    "            initial_candidates = all_candidates\n",
    "        \n",
    "        weights = config.rank_weights or [1.0] * num_embedding_types\n",
    "        \n",
    "        for idx in initial_candidates:\n",
    "            embedding_scores = []\n",
    "            valid_embeddings = 0\n",
    "            \n",
    "            for i in range(num_embedding_types):\n",
    "                distance = all_distances[i][idx]\n",
    "                \n",
    "                # Check if this embedding is an outlier\n",
    "                if abs(distance - np.mean(list(all_distances[i].values()))) < \\\n",
    "                   config.outlier_threshold * np.std(list(all_distances[i].values())):\n",
    "                    embedding_scores.append(distance * weights[i])\n",
    "                    valid_embeddings += 1\n",
    "            \n",
    "            # Only consider if we have enough valid embeddings\n",
    "            if valid_embeddings >= config.min_embeddings_required:\n",
    "                # Use median of scores to be robust to outliers\n",
    "                median_score = np.median(embedding_scores)\n",
    "                candidate_scores.append((idx, median_score))\n",
    "        \n",
    "        # Sort and get top k\n",
    "        candidate_scores.sort(key=lambda x: x[1])\n",
    "        final_indices = [idx for idx, _ in candidate_scores[:k]]\n",
    "        final_distances = [dist for _, dist in candidate_scores[:k]]\n",
    "        \n",
    "        return (np.array(final_distances).reshape(1, -1), \n",
    "                np.array(final_indices).reshape(1, -1))\n",
    "    \n",
    "    return search\n",
    "methods_list[\"robust multi index\"] =robust_multi_index_ann"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c2f00c4d0d19df4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def dimension_reduction_ann(data: EmbeddingsList, target_dim: int = 128) -> SearchFunction:\n",
    "    \"\"\"\n",
    "    Dimension reduction method. Reduces total dimensions to a target size.\n",
    "    \n",
    "    Args:\n",
    "        data: List where each entry is a list of embeddings (one per embedding type)\n",
    "        target_dim: Target dimension for the reduced space (default 128)\n",
    "    \n",
    "    Returns:\n",
    "        search_function: Function that takes query embeddings and returns distances and indices\n",
    "    \"\"\"\n",
    "    # Validate and process data\n",
    "    if not data or not all(data):\n",
    "        raise ValueError(\"Data must be non-empty and all entries must have embeddings\")\n",
    "    \n",
    "\n",
    "    # Get dimensions and validate consistency\n",
    "    num_embedding_types = len(data[0])\n",
    "    dims = [data[0][i].shape[-1] for i in range(num_embedding_types)]\n",
    "    total_original_dim = sum(dims)\n",
    "    \n",
    "    target_dim =  min(target_dim,round(total_original_dim*3 / 4 ))\n",
    "    \n",
    "    # Concatenate all data\n",
    "    concatenated_data = np.hstack([\n",
    "        np.vstack([entry[i] for entry in data])\n",
    "        for i in range(num_embedding_types)\n",
    "    ]).astype('float32')\n",
    "    \n",
    "    # Create PCA reducer\n",
    "    reducer = faiss.PCAMatrix(total_original_dim, target_dim)\n",
    "    \n",
    "    # Train reducer on the data\n",
    "    reducer.train(concatenated_data)\n",
    "    \n",
    "    # Apply reduction to the data\n",
    "    reduced_data = reducer.apply_py(concatenated_data)\n",
    "    \n",
    "    # Create and train index on reduced data\n",
    "    index = faiss.IndexHNSWFlat(target_dim, 16)\n",
    "    index.hnsw.efConstruction = 100\n",
    "    index.hnsw.efSearch = 64\n",
    "    index.add(reduced_data)\n",
    "    \n",
    "    def search(query_embeddings: List[np.ndarray], k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if len(query_embeddings) != num_embedding_types:\n",
    "            raise ValueError(f\"Expected {num_embedding_types} embedding types, got {len(query_embeddings)}\")\n",
    "        \n",
    "        # Concatenate query embeddings\n",
    "        concatenated_query = np.hstack(query_embeddings).astype('float32').reshape(1, -1)\n",
    "        \n",
    "        # Reduce query dimensions\n",
    "        reduced_query = reducer.apply_py(concatenated_query)\n",
    "        \n",
    "        # Perform search\n",
    "        distances, indices = index.search(reduced_query, k)\n",
    "        return distances, indices\n",
    "    \n",
    "    return search\n",
    "\n",
    "methods_list[\"Dimension Reduction\"] = dimension_reduction_ann"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "860ea0506cf6f127",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def normal_lsh_ann(data: EmbeddingsList, num_planes: int = 256) -> SearchFunction:\n",
    "    \"\"\"\n",
    "    Normal LSH method. Applies LSH to concatenated embeddings.\n",
    "    \n",
    "    Args:\n",
    "        data: List where each entry is a list of embeddings (one per embedding type)\n",
    "        num_planes: Number of hyperplanes for LSH\n",
    "    \n",
    "    Returns:\n",
    "        search_function: Function that takes query embeddings and returns distances and indices\n",
    "    \"\"\"\n",
    "    if not data or not all(data):\n",
    "        raise ValueError(\"Data must be non-empty and all entries must have embeddings\")\n",
    "    \n",
    "    num_embedding_types = len(data[0])\n",
    "    dims = [data[0][i].shape[-1] for i in range(num_embedding_types)]\n",
    "    total_dim = sum(dims)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    concatenated_data = np.hstack([\n",
    "        np.vstack([entry[i] for entry in data])\n",
    "        for i in range(num_embedding_types)\n",
    "    ]).astype('float32')\n",
    "    \n",
    "    # Create and train LSH index\n",
    "    index = faiss.IndexLSH(total_dim, num_planes)\n",
    "    index.train(concatenated_data)\n",
    "    index.add(concatenated_data)\n",
    "    \n",
    "    def search(query_embeddings: List[np.ndarray], k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if len(query_embeddings) != num_embedding_types:\n",
    "            raise ValueError(f\"Expected {num_embedding_types} embedding types, got {len(query_embeddings)}\")\n",
    "        \n",
    "        concatenated_query = np.hstack(query_embeddings).astype('float32').reshape(1, -1)\n",
    "        distances, indices = index.search(concatenated_query, k)\n",
    "        return distances, indices\n",
    "    \n",
    "    return search\n",
    "\n",
    "def split_lsh_ann(data: EmbeddingsList, num_planes: int = 256) -> SearchFunction:\n",
    "    \"\"\"\n",
    "    Split LSH method. Applies LSH separately to each embedding type.\n",
    "    \n",
    "    Args:\n",
    "        data: List where each entry is a list of embeddings (one per embedding type)\n",
    "        num_planes: Total number of hyperplanes to be distributed among embedding types\n",
    "    \n",
    "    Returns:\n",
    "        search_function: Function that takes query embeddings and returns distances and indices\n",
    "    \"\"\"\n",
    "    if not data or not all(data):\n",
    "        raise ValueError(\"Data must be non-empty and all entries must have embeddings\")\n",
    "    \n",
    "    num_embedding_types = len(data[0])\n",
    "    dims = [data[0][i].shape[-1] for i in range(num_embedding_types)]\n",
    "    total_dim = sum(dims)\n",
    "    \n",
    "    # Allocate planes proportionally to dimension sizes\n",
    "    planes_per_type = [max(1, int(num_planes * dim / total_dim)) for dim in dims]\n",
    "    \n",
    "    # Adjust to ensure total equals num_planes\n",
    "    while sum(planes_per_type) != num_planes:\n",
    "        if sum(planes_per_type) < num_planes:\n",
    "            idx = dims.index(max(dims))\n",
    "            planes_per_type[idx] += 1\n",
    "        else:\n",
    "            idx = dims.index(min(dims))\n",
    "            if planes_per_type[idx] > 1:\n",
    "                planes_per_type[idx] -= 1\n",
    "    \n",
    "    # Create separate LSH indexes for each embedding type\n",
    "    indexes = []\n",
    "    for i, dim in enumerate(dims):\n",
    "        index = faiss.IndexLSH(total_dim, planes_per_type[i])\n",
    "        \n",
    "        # Create masked training data\n",
    "        masked_data = np.zeros((len(data), total_dim), dtype='float32')\n",
    "        start_idx = sum(dims[:i])\n",
    "        masked_data[:, start_idx:start_idx+dims[i]] = np.vstack([entry[i] for entry in data])\n",
    "        \n",
    "        index.train(masked_data)\n",
    "        index.add(masked_data)\n",
    "        indexes.append(index)\n",
    "    \n",
    "    def search(query_embeddings: List[np.ndarray], k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if len(query_embeddings) != num_embedding_types:\n",
    "            raise ValueError(f\"Expected {num_embedding_types} embedding types, got {len(query_embeddings)}\")\n",
    "        \n",
    "        concatenated_query = np.hstack(query_embeddings).astype('float32').reshape(1, -1)\n",
    "        \n",
    "        # Combine results from all indexes\n",
    "        all_distances = []\n",
    "        all_indices = []\n",
    "        \n",
    "        for i, index in enumerate(indexes):\n",
    "            distances, indices = index.search(concatenated_query, k)\n",
    "            all_distances.append(distances)\n",
    "            all_indices.append(indices)\n",
    "        \n",
    "        # Merge and sort results\n",
    "        merged_distances = np.hstack(all_distances)\n",
    "        merged_indices = np.hstack(all_indices)\n",
    "        \n",
    "        # Sort by distance and take top k\n",
    "        sorted_indices = np.argsort(merged_distances[0])[:k]\n",
    "        final_distances = merged_distances[0][sorted_indices].reshape(1, -1)\n",
    "        final_indices = merged_indices[0][sorted_indices].reshape(1, -1)\n",
    "        \n",
    "        return final_distances, final_indices\n",
    "    \n",
    "    return search\n",
    "\n",
    "methods_list[\"Normal LSH\"] = normal_lsh_ann\n",
    "methods_list[\"Split LSH\"] = split_lsh_ann"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50cef362d941e76f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def tolerant_ann(data: EmbeddingsList, subset_size: int = None) -> SearchFunction:\n",
    "    \"\"\"\n",
    "    Outlier-tolerant ANN method. Creates multiple indexes using different subsets of embeddings.\n",
    "    Finds matches that are close in most, but not necessarily all, embedding spaces.\n",
    "    \n",
    "    Args:\n",
    "        data: List where each entry is a list of embeddings (one per embedding type)\n",
    "        subset_size: Number of embedding types to combine in each subset (default: num_types - 1)\n",
    "    \n",
    "    Returns:\n",
    "        search_function: Function that takes query embeddings and returns distances and indices\n",
    "    \"\"\"\n",
    "    if not data or not all(data):\n",
    "        raise ValueError(\"Data must be non-empty and all entries must have embeddings\")\n",
    "    \n",
    "    num_embedding_types = len(data[0])\n",
    "    dims = [data[0][i].shape[-1] for i in range(num_embedding_types)]\n",
    "    \n",
    "    if subset_size is None:\n",
    "        subset_size = max(1, num_embedding_types - 1)\n",
    "    \n",
    "    if subset_size > num_embedding_types:\n",
    "        raise ValueError(f\"subset_size ({subset_size}) cannot be larger than number of embedding types ({num_embedding_types})\")\n",
    "    \n",
    "    # Create all possible combinations of embedding types\n",
    "    from itertools import combinations\n",
    "    embedding_combinations = list(combinations(range(num_embedding_types), subset_size))\n",
    "    \n",
    "    # Create an index for each combination\n",
    "    indexes = []\n",
    "    for combo in embedding_combinations:\n",
    "        # Calculate total dimension for this combination\n",
    "        combo_dim = sum(dims[i] for i in combo)\n",
    "        \n",
    "        # Concatenate only the embeddings in this combination\n",
    "        combo_data = np.hstack([\n",
    "            np.vstack([entry[i] for entry in data])\n",
    "            for i in combo\n",
    "        ]).astype('float32')\n",
    "        \n",
    "        # Create and train index\n",
    "        index = faiss.IndexHNSWFlat(combo_dim, 16)\n",
    "        index.hnsw.efConstruction = 100\n",
    "        index.hnsw.efSearch = 64\n",
    "        index.add(combo_data)\n",
    "        \n",
    "        indexes.append((combo, index))\n",
    "    \n",
    "    def search(query_embeddings: List[np.ndarray], k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if len(query_embeddings) != num_embedding_types:\n",
    "            raise ValueError(f\"Expected {num_embedding_types} embedding types, got {len(query_embeddings)}\")\n",
    "        \n",
    "        # Search in each index and aggregate results\n",
    "        all_results = {}  # Dictionary to store index -> score mapping\n",
    "        \n",
    "        for combo, index in indexes:\n",
    "            # Concatenate only the query embeddings for this combination\n",
    "            combo_query = np.hstack([query_embeddings[i] for i in combo]).astype('float32').reshape(1, -1)\n",
    "            \n",
    "            # Search in this index\n",
    "            distances, indices = index.search(combo_query, k)\n",
    "            \n",
    "            # Accumulate scores for each found index\n",
    "            for idx, dist in zip(indices[0], distances[0]):\n",
    "                if idx not in all_results:\n",
    "                    all_results[idx] = []\n",
    "                all_results[idx].append(dist)\n",
    "        \n",
    "        # Aggregate scores using a method that rewards consistency across subsets\n",
    "        final_scores = []\n",
    "        for idx, distances in all_results.items():\n",
    "            # Use average of best scores as the final score\n",
    "            best_scores = sorted(distances)[:max(1, len(indexes) - num_embedding_types + subset_size)]\n",
    "            score = sum(best_scores) / len(best_scores)\n",
    "            final_scores.append((score, idx))\n",
    "        \n",
    "        # Sort by score and get top k\n",
    "        final_scores.sort()\n",
    "        top_k = final_scores[:k]\n",
    "        \n",
    "        # Format results to match expected output\n",
    "        result_distances = np.array([[score for score, _ in top_k]])\n",
    "        result_indices = np.array([[idx for _, idx in top_k]])\n",
    "        \n",
    "        return result_distances, result_indices\n",
    "    \n",
    "    return search\n",
    "\n",
    "methods_list[\"Tolerant ANN\"] = tolerant_ann"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4f8eb84658627b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def capped_distance_ann(data: EmbeddingsList, cap_distance: float = 0.7) -> SearchFunction:\n",
    "    \"\"\"\n",
    "    ANN method using capped distances for each embedding type.\n",
    "    Limits the impact of outlier embeddings by capping their contribution to the total distance.\n",
    "    \n",
    "    Args:\n",
    "        data: List where each entry is a list of embeddings (one per embedding type)\n",
    "        cap_distance: Maximum distance contribution from each embedding type\n",
    "    \n",
    "    Returns:\n",
    "        search_function: Function that takes query embeddings and returns distances and indices\n",
    "    \"\"\"\n",
    "    if not data or not all(data):\n",
    "        raise ValueError(\"Data must be non-empty and all entries must have embeddings\")\n",
    "    \n",
    "    num_embedding_types = len(data[0])\n",
    "    dims = [data[0][i].shape[-1] for i in range(num_embedding_types)]\n",
    "    total_dim = sum(dims)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    concatenated_data = np.hstack([\n",
    "        np.vstack([entry[i] for entry in data])\n",
    "        for i in range(num_embedding_types)\n",
    "    ]).astype('float32')\n",
    "    \n",
    "    # Create a custom index with capped distance metric\n",
    "    class CappedDistanceIndex:\n",
    "        def __init__(self, data, dims, cap):\n",
    "            self.data = data\n",
    "            self.dims = dims\n",
    "            self.cap = cap\n",
    "            self.dim_starts = [0] + list(np.cumsum(dims[:-1]))\n",
    "    \n",
    "        def search(self, query, k):\n",
    "            # Compute distances for each embedding type separately\n",
    "            n = len(self.data)\n",
    "            all_distances = np.zeros((len(query), n), dtype=np.float32)\n",
    "            \n",
    "            for i, start in enumerate(self.dim_starts):\n",
    "                end = start + self.dims[i]\n",
    "                # Compute squared L2 distance for this embedding type\n",
    "                delta = self.data[:, start:end] - query[:, start:end]\n",
    "                distances = np.sum(delta * delta, axis=1)\n",
    "                # Cap the distances\n",
    "                np.minimum(distances, self.cap * self.cap, out=distances)\n",
    "                all_distances += distances\n",
    "            \n",
    "            # Use argpartition to efficiently find top k\n",
    "            ind = np.argpartition(all_distances[0], k)[:k]\n",
    "            # Sort the top k\n",
    "            ind_sorted = ind[np.argsort(all_distances[0][ind])]\n",
    "            distances_sorted = all_distances[0][ind_sorted]\n",
    "            \n",
    "            return np.sqrt(distances_sorted.reshape(1, -1)), ind_sorted.reshape(1, -1)\n",
    "    \n",
    "    # Create the index\n",
    "    index = CappedDistanceIndex(concatenated_data, dims, cap_distance)\n",
    "    \n",
    "    def search(query_embeddings: List[np.ndarray], k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if len(query_embeddings) != num_embedding_types:\n",
    "            raise ValueError(f\"Expected {num_embedding_types} embedding types, got {len(query_embeddings)}\")\n",
    "        \n",
    "        concatenated_query = np.hstack(query_embeddings).astype('float32').reshape(1, -1)\n",
    "        distances, indices = index.search(concatenated_query, k)\n",
    "        return distances, indices\n",
    "    \n",
    "    return search\n",
    "\n",
    "methods_list[\"Capped Distance\"] = capped_distance_ann"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8a51a9778ca48f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def emphasis_close_ann(data: EmbeddingsList, boost_threshold: float = 0.3) -> SearchFunction:\n",
    "    \"\"\"\n",
    "    ANN method that emphasizes close matches in any embedding type.\n",
    "    If any embedding type shows a very close match, it reduces the impact of other distances.\n",
    "    \n",
    "    Args:\n",
    "        data: List where each entry is a list of embeddings (one per embedding type)\n",
    "        boost_threshold: Distance threshold below which a match is considered \"close\"\n",
    "                        and triggers emphasis behavior\n",
    "    \n",
    "    Returns:\n",
    "        search_function: Function that takes query embeddings and returns distances and indices\n",
    "    \"\"\"\n",
    "    if not data or not all(data):\n",
    "        raise ValueError(\"Data must be non-empty and all entries must have embeddings\")\n",
    "    \n",
    "    num_embedding_types = len(data[0])\n",
    "    dims = [data[0][i].shape[-1] for i in range(num_embedding_types)]\n",
    "    \n",
    "    # Normalize embeddings for consistent distance scaling\n",
    "    normalized_data = []\n",
    "    for i in range(num_embedding_types):\n",
    "        embeddings = np.vstack([entry[i] for entry in data])\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        normalized_data.append(embeddings / norms)\n",
    "    \n",
    "    # Create separate indexes for each embedding type for efficient search\n",
    "    indexes = []\n",
    "    for i, emb in enumerate(normalized_data):\n",
    "        index = faiss.IndexHNSWFlat(dims[i], 16)\n",
    "        index.hnsw.efConstruction = 100\n",
    "        index.hnsw.efSearch = 64\n",
    "        index.add(emb.astype('float32'))\n",
    "        indexes.append(index)\n",
    "    \n",
    "    def search(query_embeddings: List[np.ndarray], k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if len(query_embeddings) != num_embedding_types:\n",
    "            raise ValueError(f\"Expected {num_embedding_types} embedding types, got {len(query_embeddings)}\")\n",
    "        \n",
    "        # Normalize query embeddings\n",
    "        normalized_queries = []\n",
    "        for query_emb in query_embeddings:\n",
    "            norm = np.linalg.norm(query_emb)\n",
    "            normalized_queries.append((query_emb / norm).astype('float32').reshape(1, -1))\n",
    "        \n",
    "        # Initial wide search in each embedding space\n",
    "        expanded_k = min(k * 3, len(data))  # Search for more candidates initially\n",
    "        all_candidates = set()\n",
    "        per_type_results = []\n",
    "        \n",
    "        for i, (index, query) in enumerate(zip(indexes, normalized_queries)):\n",
    "            distances, indices = index.search(query, expanded_k)\n",
    "            all_candidates.update(indices[0])\n",
    "            per_type_results.append((distances[0], indices[0]))\n",
    "        \n",
    "        # Calculate final scores for all candidates\n",
    "        final_scores = []\n",
    "        for idx in all_candidates:\n",
    "            type_distances = []\n",
    "            close_matches = 0\n",
    "            \n",
    "            # Collect distances for this candidate across all embedding types\n",
    "            for type_idx, (distances, indices) in enumerate(per_type_results):\n",
    "                if idx in indices:\n",
    "                    dist = distances[np.where(indices == idx)[0][0]]\n",
    "                    if dist < boost_threshold:\n",
    "                        close_matches += 1\n",
    "                    type_distances.append(dist)\n",
    "                else:\n",
    "                    # If this candidate wasn't in top-k for this embedding type,\n",
    "                    # use a default high distance\n",
    "                    type_distances.append(1.0)\n",
    "            \n",
    "            # Calculate final score\n",
    "            # The more close matches, the more we discount other distances\n",
    "            discount_factor = 1.0 / (close_matches + 1)\n",
    "            non_close_distances = sorted([d for d in type_distances if d >= boost_threshold])\n",
    "            \n",
    "            if close_matches > 0:\n",
    "                # Use the best close distance, plus discounted sum of other distances\n",
    "                final_score = min(type_distances) + sum(non_close_distances) * discount_factor\n",
    "            else:\n",
    "                # If no close matches, use regular average\n",
    "                final_score = sum(type_distances) / len(type_distances)\n",
    "            \n",
    "            final_scores.append((final_score, idx))\n",
    "        \n",
    "        # Sort and get top k\n",
    "        final_scores.sort()\n",
    "        top_k = final_scores[:k]\n",
    "        \n",
    "        # Format results\n",
    "        result_distances = np.array([[score for score, _ in top_k]])\n",
    "        result_indices = np.array([[idx for _, idx in top_k]])\n",
    "        \n",
    "        return result_distances, result_indices\n",
    "    \n",
    "    return search\n",
    "\n",
    "methods_list[\"Emphasis Close\"] = emphasis_close_ann"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b237684baa639027",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    " # simple \n",
    "# Test all methods\n",
    "for method_name,method in methods_list.items():\n",
    "    print(f\"\\nTesting {method_name}\")\n",
    "    if True:\n",
    "        search_fn = method(database)\n",
    "        distances, indices = search_fn(queries[0])\n",
    "        \n",
    "        print(f\"Top 5 results for first query:\")\n",
    "        for i in range(1):\n",
    "            print(f\"  Index: {indices[0][i]}, Distance: {distances[0][i]:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66a3e5b3851193a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def display_images_with_labels(image1, image2, image3):\n",
    "    # Create a figure and axis\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Display images\n",
    "    axes[0].imshow(image1)\n",
    "    axes[0].set_title(\"Query image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(image2)\n",
    "    axes[1].set_title(\"Agreed similar\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(image3)\n",
    "    axes[2].set_title(\"Proposed alternative similar\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def compare_similarity_methods(methods_list, database, queries, top_k=5, result_size=5, display_score_limit = 2.5, dispplay_picture_funcation=display_images_with_labels , display_picture = None , queries_images = None):\n",
    "    results = {}\n",
    "    disagreement_scores = defaultdict(list)\n",
    "\n",
    "    # Run all methods on all queries\n",
    "    for method_name, method in methods_list.items():\n",
    "        search_fn = method(database)\n",
    "        method_results = []\n",
    "        \n",
    "        for query in queries:\n",
    "            distances, indices = search_fn(query)\n",
    "            method_results.append((distances[0][:top_k], indices[0][:top_k]))\n",
    "        \n",
    "        results[method_name] = method_results\n",
    "        \n",
    "    def spot_rank(spot: int): # gets unsigned int only . dont find out plz \n",
    "        rank = 1/(1+spot)\n",
    "        return  rank\n",
    "\n",
    "    # Compare methods and calculate disagreement scores\n",
    "    for query_idx, query in enumerate(queries):\n",
    "        rank_results = defaultdict(lambda : 0)\n",
    "        for method_name in methods_list.keys():\n",
    "            for spot , entry in enumerate( results[method_name][query_idx][1]):\n",
    "                rank_results[entry] += spot_rank(spot)\n",
    "        max_entry = max(rank_results , key=lambda r: rank_results[r])\n",
    "        max_rank = rank_results[max_entry]\n",
    "        \n",
    "        for method_name in methods_list.keys():\n",
    "            current_method_results = results[method_name][query_idx][1]\n",
    "            \n",
    "            entry_id = current_method_results[0]\n",
    "            # Calculate disagreement score\n",
    "            disagreement_score = max_rank -rank_results[entry_id]\n",
    "            \n",
    "            if disagreement_score > 0:  # Only consider cases where the method ranks an entry higher than others\n",
    "                heapq.heappush(disagreement_scores[method_name], \n",
    "                               (-disagreement_score, query_idx, max_entry, entry_id))\n",
    "                \n",
    "                # Keep only the top 'result_size' disagreements\n",
    "                if len(disagreement_scores[method_name]) > result_size:\n",
    "                    heapq.heappop(disagreement_scores[method_name])\n",
    "\n",
    "    # Print results\n",
    "    for method_name in methods_list.keys():\n",
    "        print(f\"\\nTop {result_size} disagreements for {method_name}:\")\n",
    "        max_diff = 0 \n",
    "        qi , ari , pei = 0,0,0\n",
    "        for score, query_id, agreed_result, proposed_entry in sorted(disagreement_scores[method_name], reverse=True):\n",
    "            if -score > max_diff:\n",
    "                max_diff = - score\n",
    "                qi , ari , pei = query_id, agreed_result, proposed_entry\n",
    "            print(f\"  Score difference: {-score:.2f}\")\n",
    "            print(f\"  Query ID: {query_id}\")\n",
    "            print(f\"  Agreed result ID: {agreed_result}\")\n",
    "            print(f\"  Proposed entry ID: {proposed_entry}\")\n",
    "            print()\n",
    "        if dispplay_picture_funcation is not None and display_picture is not None and queries_images is not None:\n",
    "            if display_score_limit < max_diff:\n",
    "                dispplay_picture_funcation(queries_images[qi], display_picture[ari], display_picture[pei])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "463ad4f3039c101",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# create embeddings for images \n",
    "embedding_functions = {}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "813804d1d29e4c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.cluster import KMeans\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from torchvision.models import resnet50, ResNet50_Weights , VGG16_Weights\n",
    "import torchvision.models as PTmodels\n",
    "from torchvision.models.detection import  FasterRCNN_ResNet50_FPN_Weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4334c7c05bd8ab41",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def cnn_embedding(img, model_name='vgg16'):\n",
    "    \"\"\"\n",
    "    Generate CNN features using a pre-trained model.\n",
    "    \n",
    "    Args:\n",
    "    image_path (str): Path to the input image\n",
    "    model_name (str): Name of the pre-trained model to use\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Feature vector\n",
    "    \"\"\"\n",
    "    # Load pre-trained model\n",
    "    if model_name == 'resnet50':\n",
    "        model = models.resnet50(pretrained=True)\n",
    "    elif model_name == 'vgg16':\n",
    "        model = PTmodels.vgg16(weights=PTmodels.VGG16_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name\")\n",
    "    \n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    #img = Image.open(image_path).convert('RGB')\n",
    "    # Ensure input is PIL Image\n",
    "    if not isinstance(img, Image.Image):\n",
    "        img = transforms.ToPILImage()(img)\n",
    "    img_t = transform(img)\n",
    "    batch_t = torch.unsqueeze(img_t, 0)\n",
    "    \n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(batch_t)\n",
    "    \n",
    "    return embedding.numpy().flatten()\n",
    "\n",
    "def color_palette_embedding(img, n_colors=5):\n",
    "    \"\"\"\n",
    "    Generate color palette embedding using K-means clustering.\n",
    "    \n",
    "    Args:\n",
    "    image_path (str): Path to the input image\n",
    "    n_colors (int): Number of colors in the palette\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Color palette embedding\n",
    "    \"\"\"\n",
    "    # Load and prepare image\n",
    "    #img = Image.open(image_path).convert('RGB')\n",
    "    #if not isinstance(img, np.ndarray):\n",
    "    if len(img.shape )>2:\n",
    "        img = np.array(img).reshape(-1, 3)\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_colors, random_state=42)\n",
    "    kmeans.fit(img)\n",
    "    \n",
    "    # Get color palette and normalize\n",
    "    palette = kmeans.cluster_centers_.astype(int)\n",
    "    normalized_palette = palette / 255.0\n",
    "    \n",
    "    return normalized_palette.flatten()\n",
    "\n",
    "def object_composition_embedding(img, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Generate object composition embedding using a pre-trained Faster R-CNN model.\n",
    "    \n",
    "    Args:\n",
    "    image_path (str): Path to the input image\n",
    "    threshold (float): Confidence threshold for object detection\n",
    "    \n",
    "    Returns:\n",
    "    dict: Object composition (class labels and their counts)\n",
    "    \"\"\"\n",
    "    const_number_of_class_in_fasterrcnn_resnet50_fpn  =91\n",
    "    # Load pre-trained model\n",
    "    #model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model = PTmodels.detection.fasterrcnn_resnet50_fpn(weights=PTmodels.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare image\n",
    "    #img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = to_tensor(img).unsqueeze(0)\n",
    "    \n",
    "    # Perform object detection\n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)\n",
    "    \n",
    "    # Process predictions\n",
    "    labels = prediction[0]['labels'].numpy()\n",
    "    scores = prediction[0]['scores'].numpy()\n",
    "    \n",
    "    # Filter predictions based on threshold\n",
    "    valid_preds = scores > threshold\n",
    "    detected_labels = labels[valid_preds]\n",
    "    \n",
    "    # Count occurrences of each label\n",
    "    unique, counts = np.unique(detected_labels, return_counts=True)\n",
    "    composition = [0 for _ in range(const_number_of_class_in_fasterrcnn_resnet50_fpn)]\n",
    "    for u,c in zip(unique, counts):\n",
    "        composition[u] =c\n",
    "    \n",
    "    return np.array(composition)\n",
    "\n",
    "def texture_embedding(img, distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4]):\n",
    "    \"\"\"\n",
    "    Generate texture embedding using Gray Level Co-occurrence Matrix (GLCM).\n",
    "    \n",
    "    Args:\n",
    "    image_path (str): Path to the input image\n",
    "    distances (list): List of pixel pair distance offsets\n",
    "    angles (list): List of pixel pair angles in radians\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Texture features\n",
    "    \"\"\"\n",
    "    # Load and prepare image\n",
    "    if not isinstance(img, np.ndarray):\n",
    "        img = img.convert('L')  # Convert to grayscale\n",
    "        img = np.array(img)\n",
    "    if len(img.shape) >2:\n",
    "        img = np.array(img).reshape(-1, 3)\n",
    "    if np.issubdtype(img.dtype, np.floating):\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "    # Compute GLCM\n",
    "    glcm = graycomatrix(img, distances=distances, angles=angles, \n",
    "                        levels=256, symmetric=True, normed=True)\n",
    "    \n",
    "    # Compute GLCM properties\n",
    "    contrast = graycoprops(glcm, 'contrast')\n",
    "    dissimilarity = graycoprops(glcm, 'dissimilarity')\n",
    "    homogeneity = graycoprops(glcm, 'homogeneity')\n",
    "    energy = graycoprops(glcm, 'energy')\n",
    "    correlation = graycoprops(glcm, 'correlation')\n",
    "    \n",
    "    # Combine features\n",
    "    features = np.hstack([contrast, dissimilarity, homogeneity, energy, correlation])\n",
    "    \n",
    "    return features.flatten()\n",
    "\n",
    "embedding_functions[\"cnn_embedding\"] = cnn_embedding\n",
    "embedding_functions[\"color_palette_embedding\"] = color_palette_embedding\n",
    "embedding_functions[\"object_composition_embedding\"] = object_composition_embedding\n",
    "embedding_functions[\"texture_embedding\"] = texture_embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da218d7479357ed4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def orb_embedding(img, n_features=100): # image_path\n",
    "    \"\"\"\n",
    "    Generate ORB (Oriented FAST and Rotated BRIEF) features for an image.\n",
    "    \n",
    "    Args:\n",
    "    image_path (str): Path to the input image\n",
    "    n_features (int): Maximum number of features to extract\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Fixed-length feature vector combining descriptor information\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    \n",
    "    # Read image in grayscale\n",
    "    #img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if not isinstance(img, np.ndarray):\n",
    "        img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    if img is None:\n",
    "        raise ValueError(\"Could not load image\")\n",
    "    \n",
    "    # Initialize ORB detector\n",
    "    orb = cv2.ORB_create(nfeatures=n_features)\n",
    "    \n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = orb.detectAndCompute(img, None)\n",
    "    \n",
    "    if descriptors is None:\n",
    "        # Return zero vector if no features found\n",
    "        return np.zeros(n_features * 8, dtype=np.float32)\n",
    "    \n",
    "    # Ensure we have a fixed-length output\n",
    "    if len(keypoints) < n_features:\n",
    "        # Pad with zeros if we found fewer features than requested\n",
    "        padding = np.zeros((n_features - len(keypoints), 32), dtype=np.uint8)\n",
    "        descriptors = np.vstack((descriptors, padding))\n",
    "    elif len(keypoints) > n_features:\n",
    "        # Take only the first n_features if we found more\n",
    "        descriptors = descriptors[:n_features]\n",
    "    \n",
    "    # Convert binary descriptors to float and flatten\n",
    "    descriptors = descriptors.astype(np.float32)\n",
    "    \n",
    "    # Normalize the descriptors\n",
    "    descriptors = descriptors / 255.0\n",
    "    \n",
    "    # Flatten the descriptor array\n",
    "    flattened = descriptors.flatten()\n",
    "    \n",
    "    return flattened\n",
    "\n",
    "\n",
    "embedding_functions[\"orb_embedding\"] = orb_embedding\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b059e22aaf4d02c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "from sklearn.cluster import KMeans\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage.measure import shannon_entropy\n",
    "from skimage.filters import sobel\n",
    "from skimage.transform import hough_line, hough_line_peaks\n",
    "import cv2\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "642920c3eb0f93d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Helper embedding functions\n",
    "def resnet18_embedding(img, layer='avgpool'):\n",
    "    \"\"\"\n",
    "    Compute ResNet18 embedding for an image\n",
    "    \n",
    "    Args:\n",
    "    img (PIL.Image or torch.Tensor): Input image\n",
    "    layer (str): Layer to extract embedding from\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: Flattened embedding vector\n",
    "    \"\"\"\n",
    "    # Load pretrained ResNet18\n",
    "    model = PTmodels.resnet18(weights=PTmodels.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    #if not isinstance(img, np.ndarray):\n",
    "        # Ensure input is PIL Image\n",
    "    if not isinstance(img, Image.Image):\n",
    "        img = transforms.ToPILImage()(img)\n",
    "            \n",
    "            # Preprocess image\n",
    "    input_tensor = preprocess(img).unsqueeze(0)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        if layer == 'avgpool':\n",
    "            # Use average pooling layer\n",
    "            features = model.avgpool(model.layer4(model.layer3(model.layer2(model.layer1(model.conv1(input_tensor))))))\n",
    "        elif layer == 'fc':\n",
    "            # Use final fully connected layer\n",
    "            features = model.fc(model.avgpool(model.layer4(model.layer3(model.layer2(model.layer1(model.conv1(input_tensor))))).flatten(1)))\n",
    "    \n",
    "    return features.squeeze().numpy().flatten()\n",
    "\n",
    "def simple_color_histogram_embedding(img, bins=32):\n",
    "    \"\"\"\n",
    "    Compute a simple color histogram embedding\n",
    "    \n",
    "    Args:\n",
    "    img (PIL.Image or torch.Tensor): Input image\n",
    "    bins (int): Number of bins for color histogram\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: Flattened color histogram\n",
    "    \"\"\"\n",
    "    if not isinstance(img, np.ndarray):\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = transforms.ToPILImage()(img)\n",
    "        img = np.array(img)\n",
    "    \n",
    "    # Compute histogram for each color channel\n",
    "    hist_r = np.histogram(img[:,:,0], bins=bins, range=[0,256])[0]\n",
    "    hist_g = np.histogram(img[:,:,1], bins=bins, range=[0,256])[0]\n",
    "    hist_b = np.histogram(img[:,:,2], bins=bins, range=[0,256])[0]\n",
    "    \n",
    "    # Normalize and concatenate\n",
    "    hist = np.concatenate([\n",
    "        hist_r / np.sum(hist_r),\n",
    "        hist_g / np.sum(hist_g),\n",
    "        hist_b / np.sum(hist_b)\n",
    "    ])\n",
    "    \n",
    "    return hist\n",
    "\n",
    "embedding_functions[\"resnet18_embedding\"]=resnet18_embedding\n",
    "embedding_functions[\"simple_color_histogram_embedding\"]=simple_color_histogram_embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6500c91d857bb13",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def composition_rules_embedding(img):#image_path):\n",
    "    \"\"\"\n",
    "    Generate embedding based on basic photography composition rules.\n",
    "    \n",
    "    Args:\n",
    "    image_path (str): Path to the input image\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Composition features\n",
    "    \"\"\"\n",
    "    #img = np.array(Image.open(image_path).convert('RGB'))\n",
    "    if not isinstance(img, np.ndarray):\n",
    "        img = np.array(img.convert('RGB'))\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Rule of thirds points\n",
    "    third_h = h // 3\n",
    "    third_w = w // 3\n",
    "    \n",
    "    # Calculate feature importance at rule of thirds intersections\n",
    "    thirds_points = [\n",
    "        img[third_h, third_w],\n",
    "        img[third_h, 2*third_w],\n",
    "        img[2*third_h, third_w],\n",
    "        img[2*third_h, 2*third_w]\n",
    "    ]\n",
    "    \n",
    "    # Center weight\n",
    "    center_region = img[h//3:2*h//3, w//3:2*w//3]\n",
    "    center_weight = np.mean(center_region)\n",
    "    \n",
    "    # Convert to features\n",
    "    thirds_features = np.mean(thirds_points, axis=1)\n",
    "    \n",
    "    return np.concatenate([thirds_features.flatten(), [center_weight]])\n",
    "\n",
    "def scene_complexity_embedding(img):#image_path):\n",
    "    \"\"\"\n",
    "    Generate embedding based on image complexity metrics.\n",
    "    \n",
    "    Args:\n",
    "    image_path (str): Path to the input image\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Complexity features\n",
    "    \"\"\"\n",
    "    #img = np.array(Image.open(image_path).convert('L'))\n",
    "    if not isinstance(img, np.ndarray):\n",
    "        img = np.array(Image.convert('L'))\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = shannon_entropy(img)\n",
    "    \n",
    "    # Calculate frequency domain features\n",
    "    f_transform = np.fft.fft2(img)\n",
    "    f_spectrum = np.abs(np.fft.fftshift(f_transform))\n",
    "    freq_energy = np.sum(f_spectrum)\n",
    "    \n",
    "    # Calculate number of unique intensity values\n",
    "    unique_intensities = len(np.unique(img))\n",
    "    \n",
    "    # Calculate local variance\n",
    "    local_var = np.std(img)\n",
    "    \n",
    "    return np.array([entropy, freq_energy, unique_intensities, local_var])\n",
    "\n",
    "def semantic_concept_embedding(image):#image_path):\n",
    "    \"\"\"\n",
    "    Generate embedding based on high-level semantic concepts using CLIP.\n",
    "    \n",
    "    Args:\n",
    "    image_path (str): Path to the input image\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Semantic concept features\n",
    "    \"\"\"\n",
    "    # Load CLIP model and processor\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    # Prepare image\n",
    "    #image = Image.open(image_path)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    \n",
    "    return image_features.numpy()[0]\n",
    "\n",
    "embedding_functions[\"composition_rules_embedding\"] = composition_rules_embedding\n",
    "embedding_functions[\"scene_complexity_embedding\"] = scene_complexity_embedding\n",
    "embedding_functions[\"semantic_concept_embedding\"] = semantic_concept_embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64f93785a5cb7d37",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def get_sample_images(n_images=25):\n",
    "    \"\"\"Get n_images unique images from CIFAR100 dataset\"\"\"\n",
    "    # Load CIFAR100 dataset\n",
    "    dataset = CIFAR100(root='./data', train=True, download=True)\n",
    "    \n",
    "    # Randomly select n_images unique indices\n",
    "    selected_indices = random.sample(range(len(dataset)), n_images)\n",
    "    \n",
    "    # Get the images\n",
    "    images = []\n",
    "    for idx in selected_indices:\n",
    "        img, _ = dataset[idx]\n",
    "        images.append(img)\n",
    "    \n",
    "    return images\n",
    "\n",
    "def display_image_grid(images, n_rows=5, n_cols=5):\n",
    "    \"\"\"Display images in a grid\"\"\"\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for idx, image in enumerate(images):\n",
    "        if idx >= n_rows * n_cols:\n",
    "            break\n",
    "        plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def apply_rotation_changes(image):\n",
    "    \"\"\"Apply different levels of rotation\"\"\"\n",
    "    transforms_list = [\n",
    "        transforms.RandomRotation(degrees=(0, 0)),  # No rotation\n",
    "        transforms.RandomRotation(degrees=(2, 2)),  # Slight rotation\n",
    "        transforms.RandomRotation(degrees=(90, 90)), # Moderate rotation\n",
    "        transforms.RandomRotation(degrees=(60, 60))  # Severe rotation\n",
    "    ]\n",
    "    return [transform(image) for transform in transforms_list]\n",
    "\n",
    "def apply_brightness_changes(image):\n",
    "    \"\"\"Apply different levels of brightness adjustment\"\"\"\n",
    "    transforms_list = [\n",
    "        transforms.ColorJitter(brightness=0),  # Original\n",
    "        transforms.ColorJitter(brightness=0.2),  # Slight brightness change\n",
    "        transforms.ColorJitter(brightness=0.5),  # Moderate brightness change\n",
    "        transforms.ColorJitter(brightness=0.8)   # Severe brightness change\n",
    "    ]\n",
    "    return [transform(image) for transform in transforms_list]\n",
    "\n",
    "def apply_noise_changes(image):\n",
    "    \"\"\"Apply different levels of Gaussian noise\"\"\"\n",
    "    def add_gaussian_noise(img, std):\n",
    "        img_array = np.array(img)\n",
    "        noise = np.random.normal(0, std, img_array.shape)\n",
    "        noisy_img = np.clip(img_array + noise, 0, 255).astype(np.uint8)\n",
    "        return Image.fromarray(noisy_img)\n",
    "    \n",
    "    return [\n",
    "        image,  # Original\n",
    "        add_gaussian_noise(image, 10),  # Light noise\n",
    "        add_gaussian_noise(image, 25),  # Medium noise\n",
    "        add_gaussian_noise(image, 50)   # Heavy noise\n",
    "    ]\n",
    "\n",
    "def apply_blur_changes(image):\n",
    "    \"\"\"Apply different levels of Gaussian blur\"\"\"\n",
    "    transforms_list = [\n",
    "        transforms.GaussianBlur(kernel_size=1, sigma=0.1),  # No blur\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=1.0),  # Light blur\n",
    "        transforms.GaussianBlur(kernel_size=5, sigma=2.0),  # Medium blur\n",
    "        transforms.GaussianBlur(kernel_size=7, sigma=3.0)   # Heavy blur\n",
    "    ]\n",
    "    return [transform(image) for transform in transforms_list]\n",
    "\n",
    "def create_transformation_dataset(images):\n",
    "    \"\"\"Create dataset with all transformation levels for each image\"\"\"\n",
    "    all_transformed_images = {}\n",
    "    \n",
    "    transformation_functions = {\"rotation\": apply_rotation_changes,\n",
    "                                \"brightness\":apply_brightness_changes,\n",
    "                                \"noise\":apply_noise_changes,\n",
    "                                \"blur\":apply_blur_changes,\n",
    "                                }\n",
    "    for transformation_name ,transform_func in transformation_functions.items():\n",
    "        result = []\n",
    "        for idx, image in enumerate(images):\n",
    "            # Select transformation function based on image index\n",
    "            transformed_images = transform_func(image)\n",
    "            result.extend(transformed_images)\n",
    "        all_transformed_images[transformation_name] = result\n",
    "    \n",
    "    return all_transformed_images\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77bc7e1ce44ac13c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "\n",
    "# Get 25 sample images\n",
    "original_images = get_sample_images(25)\n",
    "\n",
    "# Display original images in 5x5 grid\n",
    "display_image_grid(original_images[:9])\n",
    "\n",
    "# Create transformed dataset\n",
    "transformed_dataset = create_transformation_dataset(original_images)\n",
    "\n",
    "# Display first 25 transformed images (you can modify this to view different subsets)\n",
    "display_image_grid(transformed_dataset[\"noise\"][:8])\n",
    "\n",
    "#transformed_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74374842851ffbc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def compute_embedding_scores(embeddings):\n",
    "    \"\"\"\n",
    "    Compute scores for transformed images based on their embedding distances\n",
    "    \n",
    "    Args:\n",
    "    embeddings (torch.Tensor or np.ndarray): 100 image embeddings\n",
    "    \n",
    "    Returns:\n",
    "    list: Scores for each set of 4 transformed images\n",
    "    \"\"\"\n",
    "    # Ensure embeddings are numpy array\n",
    "    if torch.is_tensor(embeddings):\n",
    "        embeddings = embeddings.numpy()\n",
    "    \n",
    "    # Number of sets (25 original images * 4 transformation groups)\n",
    "    num_sets = len(embeddings) // 4 # 25 \n",
    "    scores = []\n",
    "    \n",
    "    for set_idx in range(num_sets):\n",
    "        # Select the base (original) image embedding\n",
    "        base_index = set_idx * 4\n",
    "        \n",
    "        \n",
    "        # Select the current set of 4 embeddings\n",
    "        current_set_embeddings = embeddings[base_index:base_index+4]\n",
    "        base_embedding = current_set_embeddings[0]\n",
    "        \n",
    "        # Compute distances between base embedding and all 100 embeddings\n",
    "        distances = cdist([base_embedding], embeddings, metric='cosine')[0]\n",
    "        \n",
    "        # Compute ranks for each transformation\n",
    "        ranks = []\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        # Find the rank of the original images in this set\n",
    "        for i in range(base_index,base_index+4):\n",
    "            ranks.append(np.where(sorted_indices == i)[0][0])\n",
    "        \n",
    "        # Advanced scoring method\n",
    "        # Exponential penalty for higher ranks with non-linear decay\n",
    "        score = (\n",
    "            4 * np.exp(-ranks[0]/10) +  # Original image\n",
    "            3 * np.exp(-ranks[1]/20) +  # Lightly transformed\n",
    "            2 * np.exp(-ranks[2]/30) +  # Moderately transformed\n",
    "            1 * np.exp(-ranks[3]/50)    # Heavily transformed\n",
    "        )\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "781de5d97635c241",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "def measure_embedding_times(image, embedding_functions: dict):\n",
    "    for name, func in embedding_functions.items():\n",
    "        print(\"for funcation :\", name)\n",
    "        start_time = time.time()\n",
    "        func(np.array(image))\n",
    "        end_time = time.time()\n",
    "        print(\"the time is:\", end_time - start_time)\n",
    "\n",
    "measure_embedding_times(original_images[0] , embedding_functions )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb4144486b57c281",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def comprehensive_embedding_analysis(\n",
    "    image_transformations, \n",
    "    embedding_functions, \n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze embedding performance across different transformation types and embedding methods\n",
    "    \n",
    "    Args:\n",
    "    image_transformations (dict): {transform_name: [100 images]}\n",
    "    embedding_functions (dict): {embedding_name: callable embedding function}\n",
    "    verbose (bool): Whether to print detailed results\n",
    "    \n",
    "    Returns:\n",
    "    dict: Comprehensive analysis results\n",
    "    \"\"\"\n",
    "    # Final results storage\n",
    "    analysis_results = {}\n",
    "    \n",
    "    # Iterate through each embedding method\n",
    "    for embed_name, embed_func in embedding_functions.items():\n",
    "        # Store results for this embedding method\n",
    "        embed_results = {}\n",
    "        \n",
    "        # Iterate through each transformation type\n",
    "        for transform_name, images in image_transformations.items():\n",
    "            # Convert images to numpy array of embeddings\n",
    "            try:\n",
    "                embeddings = np.array([\n",
    "                    embed_func(img) for img in images\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                print(f\"Error embedding {transform_name} with {embed_name}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Compute embedding scores\n",
    "            try:\n",
    "                scores = compute_embedding_scores(embeddings)\n",
    "                \n",
    "                # Compute statistics\n",
    "                mean_score = np.mean(scores)\n",
    "                std_score = np.std(scores)\n",
    "                \n",
    "                # Store results\n",
    "                embed_results[transform_name] = {\n",
    "                    'scores': scores,\n",
    "                    'mean_score': mean_score,\n",
    "                    'std_score': std_score\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing scores for {transform_name} with {embed_name}: {e}\")\n",
    "        \n",
    "        # Store embedding method results\n",
    "        analysis_results[embed_name] = embed_results\n",
    "    \n",
    "    # Verbose printing\n",
    "    if verbose:\n",
    "        print(\"\\n===== Embedding Analysis Results =====\")\n",
    "        for embed_name, embed_results in analysis_results.items():\n",
    "            print(f\"\\n{embed_name} Embedding Analysis:\")\n",
    "            for transform_name, result in embed_results.items():\n",
    "                print(f\"  {transform_name}:\")\n",
    "                print(f\"    Mean Score: {result['mean_score']:.4f}\")\n",
    "                print(f\"    Std Dev:    {result['std_score']:.4f}\")\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "def example_usage(embedding_functions,image_transformations):\n",
    "    \"\"\"\n",
    "    Example of how to use comprehensive embedding analysis\n",
    "    \n",
    "    Args:\n",
    "    image_transformations (dict): Dictionary of image transformations\n",
    "    \"\"\"\n",
    "    # Perform analysis\n",
    "    results = comprehensive_embedding_analysis(\n",
    "        image_transformations, \n",
    "        embedding_functions\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Assuming previous code for image transformations exists\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    output = {'cnn_embedding': {'rotation': {'mean_score': 9.279951468969303, 'std_score': 0.5228738273544796}, 'brightness': {'mean_score': 9.619479076729254, 'std_score': 0.06287745571148899}, 'noise': {'mean_score': 9.129639231631808, 'std_score': 0.4850666314145876}, 'blur': {'mean_score': 9.652409504800124, 'std_score': 0.06516211199870199}}, 'color_palette_embedding': {'rotation': {'mean_score': 7.0150218676586, 'std_score': 1.176705144422394}, 'brightness': {'mean_score': 8.712775297566337, 'std_score': 1.0907352053398691}, 'noise': {'mean_score': 7.7002460606126, 'std_score': 1.5819974433642798}, 'blur': {'mean_score': 7.981503805928394, 'std_score': 1.3263521120918198}}, 'object_composition_embedding': {'rotation': {'mean_score': 3.086884624261154, 'std_score': 2.8514783444668628}, 'brightness': {'mean_score': 3.112322959102469, 'std_score': 2.8862478155235403}, 'noise': {'mean_score': 2.718949958295171, 'std_score': 2.2813804200135217}, 'blur': {'mean_score': 2.811589758475318, 'std_score': 2.3897654189664537}}, 'texture_embedding': {'rotation': {'mean_score': 7.017257251536493, 'std_score': 0.968019802142182}, 'brightness': {'mean_score': 9.589879223913742, 'std_score': 0.0825360389506324}, 'noise': {'mean_score': 8.14887967219288, 'std_score': 0.943276245624005}, 'blur': {'mean_score': 8.04787621205756, 'std_score': 1.6008730912006686}}, 'orb_embedding': {'rotation': {'mean_score': 2.135176705559059, 'std_score': 2.1090008976254984}, 'brightness': {'mean_score': 2.135176705559059, 'std_score': 2.1090008976254984}, 'noise': {'mean_score': 2.135176705559059, 'std_score': 2.1090008976254984}, 'blur': {'mean_score': 2.135176705559059, 'std_score': 2.1090008976254984}}, 'resnet18_embedding': {'rotation': {'mean_score': 9.545184057802482, 'std_score': 0.24819643985265794}, 'brightness': {'mean_score': 8.484506255217056, 'std_score': 0.9845392175554086}, 'noise': {'mean_score': 8.60332950904624, 'std_score': 0.5746430131022944}, 'blur': {'mean_score': 9.298168605217189, 'std_score': 0.5691158240400043}}, 'simple_color_histogram_embedding': {'rotation': {'mean_score': 9.422409803329568, 'std_score': 0.17054289277673787}, 'brightness': {'mean_score': 8.144029551066467, 'std_score': 1.268061064579965}, 'noise': {'mean_score': 9.474449473405622, 'std_score': 0.2165506856022929}, 'blur': {'mean_score': 9.64832012123764, 'std_score': 0.04131197998927866}}, 'composition_rules_embedding': {'rotation': {'mean_score': 7.940926598618239, 'std_score': 0.7697549086658149}, 'brightness': {'mean_score': 9.573509926856365, 'std_score': 0.07961568110678785}, 'noise': {'mean_score': 9.504592599260107, 'std_score': 0.18506826100322477}, 'blur': {'mean_score': 9.424750032396684, 'std_score': 0.3663873556989816}}, 'scene_complexity_embedding': {'rotation': {'mean_score': 7.599703335612207, 'std_score': 1.0356749158247425}, 'brightness': {'mean_score': 8.236728590414288, 'std_score': 0.8341092305914172}, 'noise': {'mean_score': 6.8927261455636595, 'std_score': 0.9460073418551256}, 'blur': {'mean_score': 5.223397028017563, 'std_score': 1.0596380864286603}}, 'semantic_concept_embedding': {'rotation': {'mean_score': 9.430091119057082, 'std_score': 0.2928156456935988}, 'brightness': {'mean_score': 9.599496867957804, 'std_score': 0.11692920739396973}, 'noise': {'mean_score': 8.572574984770254, 'std_score': 1.1324747931842993}, 'blur': {'mean_score': 9.202430635899812, 'std_score': 0.45761894948833565}}}\n",
    "    #output = None\n",
    "    if output is None: # this takes 1 hour\n",
    "        results = example_usage(embedding_functions, transformed_dataset)\n",
    "    else:\n",
    "        print(\"\\n===== Embedding Analysis Results =====\")\n",
    "        for embed_name, embed_results in output.items():\n",
    "            print(f\"\\n{embed_name} Embedding Analysis:\")\n",
    "            for transform_name, result in embed_results.items():\n",
    "                print(f\"  {transform_name}:\")\n",
    "                print(f\"    Mean Score: {result['mean_score']:.4f}\")\n",
    "                print(f\"    Std Dev:    {result['std_score']:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15863c15aa76636d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "good= [\"simple_color_histogram_embedding\" ,\"composition_rules_embedding\", \"color_palette_embedding\",\"resnet18_embedding\",\"texture_embedding\" , \"cnn_embedding\",\"semantic_concept_embedding\"]\n",
    "good_embedding_functions = {}\n",
    "for key ,item in embedding_functions.items():\n",
    "    if key in good:\n",
    "        good_embedding_functions[key] = item"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "238bc12bf360b69b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def measure_embedding_times(image_list: Image.Image, embedding_functions: dict):\n",
    "    for name, func in embedding_functions.items():\n",
    "        print(\"for funcation :\", name)\n",
    "        start_time = time.time()\n",
    "        for image in image_list[:5]:\n",
    "            func(np.array(image))\n",
    "        end_time = time.time()\n",
    "        print(\"the time is:\", end_time - start_time)\n",
    "\n",
    "measure_embedding_times(original_images , good_embedding_functions )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "575040dd07fe19ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "too_slow = [ \"cnn_embedding\", \"semantic_concept_embedding\"]\n",
    "for key in too_slow:\n",
    "    good_embedding_functions.pop(key)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43479eb61f122d97",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "###### start heavy testing "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac35e08b7e85127b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def get_all_embedings(image , embedding_functions = good_embedding_functions):\n",
    "    return [ ef(image) for ef in embedding_functions.values()]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7123f865e0030db6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ],
   "id": "c898abbd65c9b11a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def download_tiny_imagenet(base_dir):\n",
    "    \"\"\"\n",
    "    Downloads and extracts Tiny ImageNet dataset\n",
    "    \"\"\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "    zip_path = os.path.join(base_dir, \"tiny-imagenet-200.zip\")\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(\"Downloading Tiny ImageNet...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            for data in tqdm(response.iter_content(chunk_size=1024), total=total_size // 1024):\n",
    "                f.write(data)\n",
    "\n",
    "    if not os.path.exists(os.path.join(base_dir, \"tiny-imagenet-200\")):\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(base_dir)\n",
    "\n",
    "\n",
    "def get_similar_image_pairs_tiny(num_pairs,pair_size = 2,  base_dir=\"./dataset\", similarity_threshold=0.7, image_size=64):\n",
    "    \"\"\"\n",
    "    Returns pairs of similar Tiny ImageNet images where pairs are from the same class,\n",
    "    with additional similarity check for better matching.\n",
    "\n",
    "    Args:\n",
    "        num_pairs (int): Number of pairs to return\n",
    "        base_dir (str): Directory to store/load dataset\n",
    "        similarity_threshold (float): Minimum similarity for paired images\n",
    "        image_size (int): Size to resize images to (Tiny ImageNet images are 64x64)\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Selected images array of shape (num_pairs*2, image_size, image_size, 3)\n",
    "        list: Class names for each pair\n",
    "    \"\"\"\n",
    "    # Download and extract dataset if needed\n",
    "    download_tiny_imagenet(base_dir)\n",
    "\n",
    "    # Create feature extractor for similarity checks\n",
    "    feature_extractor = tf.keras.applications.MobileNetV2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(image_size, image_size, 3),\n",
    "        pooling='avg'\n",
    "    )\n",
    "\n",
    "    # Path to training images\n",
    "    train_dir = os.path.join(base_dir, \"tiny-imagenet-200\", \"train\")\n",
    "\n",
    "    # Dictionary to store images by class\n",
    "    class_images = {}\n",
    "    class_features = {}\n",
    "    selected_images = []\n",
    "    selected_class_names = []\n",
    "    used_classes = set()\n",
    "\n",
    "    def load_and_preprocess_image(image_path):\n",
    "        \"\"\"Load and preprocess image for feature extraction\"\"\"\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (image_size, image_size))\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        return img, tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "\n",
    "    # Load class data\n",
    "    print(\"Loading dataset...\")\n",
    "    class_dirs = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
    "\n",
    "    class_dirs = random.sample(class_dirs, num_pairs)\n",
    "\n",
    "    for class_dir in tqdm(class_dirs):\n",
    "        class_path = os.path.join(train_dir, class_dir)\n",
    "        image_files = [f for f in os.listdir(os.path.join(class_path, \"images\"))\n",
    "                       if f.endswith('.JPEG')][:min(500 , 10+  pair_size) ]  # Take up to 10 images per class\n",
    "\n",
    "        if len(image_files) >= pair_size:  # Only include classes with at least 2 images\n",
    "            class_images[class_dir] = []\n",
    "            class_features[class_dir] = []\n",
    "\n",
    "            for img_file in image_files:\n",
    "                img_path = os.path.join(class_path, \"images\", img_file)\n",
    "                original_img, processed_img = load_and_preprocess_image(img_path)\n",
    "                features = feature_extractor.predict(tf.expand_dims(processed_img, 0), verbose=0)\n",
    "\n",
    "                class_images[class_dir].append(original_img)\n",
    "                class_features[class_dir].append(features.flatten())\n",
    "\n",
    "    print(\"Finding similar pairs...\")\n",
    "    \"\"\"\n",
    "    for _ in tqdm(range(num_pairs)):\n",
    "        # Find an unused class\n",
    "        available_classes = [c for c in class_images.keys()\n",
    "                             if c not in used_classes and len(class_images[c]) >= pair_size]\n",
    "\n",
    "        if not available_classes:\n",
    "            raise ValueError(f\"Not enough unique classes with sufficient images. \"\n",
    "                             f\"Only found {len(selected_images) // pair_size} pairs.\")\n",
    "\n",
    "        # Pick a random unused class\n",
    "        chosen_class = np.random.choice(available_classes)\n",
    "        \"\"\"\n",
    "    for chosen_class in class_images.keys():\n",
    "        if len(class_images[chosen_class]) <= pair_size:\n",
    "            raise ValueError(f\"Not enough unique classes with sufficient images. \"\n",
    "                             f\"Only found {len(selected_images) // pair_size} pairs.\")\n",
    "        used_classes.add(chosen_class)\n",
    "\n",
    "        # Get features for this class\n",
    "        class_feat = class_features[chosen_class]\n",
    "        class_imgs = class_images[chosen_class]\n",
    "\n",
    "        # Pick first image randomly\n",
    "        first_idx = np.random.randint(len(class_imgs))\n",
    "        first_features = class_feat[first_idx]\n",
    "\n",
    "        # Find most similar image\n",
    "        similarities = [1 - cosine(first_features, feat) for feat in class_feat]\n",
    "        similarities[first_idx] = -1  # Exclude the same image\n",
    "\n",
    "        selected_class_names.append(chosen_class)\n",
    "        selected_images.extend([class_imgs[first_idx].numpy()])\n",
    "        #second_idx = np.argmax(similarities)\n",
    "        pair_inxs = np.argpartition(similarities, pair_size - 1 )[ -(pair_size-1):]\n",
    "        # Add the pair to our selection\n",
    "\n",
    "        for second_idx in pair_inxs:\n",
    "            selected_images.extend([ class_imgs[second_idx].numpy()])\n",
    "\n",
    "\n",
    "    return np.array(selected_images), selected_class_names\n",
    "\n",
    "\n",
    "def visualize_tiny_imagenet_pairs(images, class_names ,pair_size = 2  ):\n",
    "    \"\"\"\n",
    "    Helper function to display the image pairs side by side with their class names\n",
    "\n",
    "    Args:\n",
    "        images (numpy array): Array of images to display\n",
    "        class_names (list): List of class names for each pair\n",
    "    \"\"\"\n",
    "\n",
    "    num_pairs = len(images) // pair_size\n",
    "    fig, axes = plt.subplots(num_pairs, pair_size, figsize=(10, 5 * num_pairs))\n",
    "\n",
    "    if num_pairs == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i in range(num_pairs):\n",
    "        for j in range(pair_size):\n",
    "            idx = i * pair_size + j\n",
    "            axes[i, j].imshow(images[idx].astype('uint8'))\n",
    "            axes[i, j].axis('off')\n",
    "            if j == 0:\n",
    "                axes[i, j].set_title(f'Class: {class_names[i]}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get 3 pairs of similar images (6 images total)\n",
    "images, class_names = get_similar_image_pairs_tiny(2 ,pair_size =4)\n",
    "\n",
    "# Visualize the pairs with their class names\n",
    "visualize_tiny_imagenet_pairs(images, class_names , pair_size =4 )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b623609cecf061a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def display_images_with_labels_nparray(image1, image2, image3):\n",
    "    # Create a figure and axis\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Display images\n",
    "    axes[0].imshow(image1.astype('uint8'))\n",
    "    axes[0].set_title(\"Query image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(image2.astype('uint8'))\n",
    "    axes[1].set_title(\"Agreed similar\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(image3.astype('uint8'))\n",
    "    axes[2].set_title(\"Proposed alternative similar\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b56f203a6ead80c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "numer_of_images_per_class = 8 # todo make this very big",
   "id": "6b42edaff37bb0c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "pair_similar_images, _ = get_similar_image_pairs_tiny(200 ,pair_size = numer_of_images_per_class)",
   "metadata": {
    "collapsed": false
   },
   "id": "d54e6d498e701c90",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "start_time = time.time()\n",
    "pair_sample_embd = [  get_all_embedings(i) for i in pair_similar_images]\n",
    "print( time.time() - start_time)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abba2b13f04aefb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from typing import Dict, Callable, Tuple, List\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def evaluate_ann_search(\n",
    "    embedded_images: List[np.ndarray],\n",
    "    ann_methods: Dict[str, Callable],\n",
    "    num_searches: int,\n",
    "    pair_size: int = 2,\n",
    "    K: int = 20,\n",
    "    display = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates the performance of different ANN search functions on the provided embedded images.\n",
    "\n",
    "    Args:\n",
    "        embedded_images (List[np.ndarray]): List of embedded images, where each element is a\n",
    "                                            numpy array representing the embeddings for one image.\n",
    "        ann_methods (Dict[str, Callable]): Dictionary of ANN search functions, where the\n",
    "                                           keys are the method names and the values are the\n",
    "                                           search functions.\n",
    "        num_searches (int): Number of searches to perform for each ANN method.\n",
    "        pair_size (int, optional): Number of images per pair. Defaults to 2.\n",
    "        K (int, optional): Number of nearest neighbors to retrieve. Defaults to 20.\n",
    "        display (bool, optional): Whether to display additional information. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Comprehensive performance metrics for each ANN search method\n",
    "    \"\"\"\n",
    "    # Define comprehensive columns for performance metrics\n",
    "    columns = [\n",
    "        'Method Name',\n",
    "        'Index Build Time',\n",
    "        'Average Search Time',\n",
    "        'Accuracy@K',\n",
    "        'Recall@K',\n",
    "        'Mean Average Precision@K',\n",
    "        'Diversity Score'\n",
    "    ]\n",
    "\n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "\n",
    "    # Ensure number of searches doesn't exceed available data\n",
    "    num_searches = min(num_searches, len(embedded_images) // pair_size)\n",
    "\n",
    "    # Randomly select search pairs\n",
    "    pair_indices = np.random.choice(\n",
    "        range(len(embedded_images) // pair_size),\n",
    "        size=num_searches,\n",
    "        replace=False\n",
    "    ) * pair_size\n",
    "\n",
    "    # Iterate through each ANN method\n",
    "    for method_name, ANN_func in ann_methods.items():\n",
    "        print(f\"Evaluating method: {method_name}\")\n",
    "\n",
    "        # Performance tracking variables\n",
    "        search_times = []\n",
    "        accuracies = []\n",
    "        recalls = []\n",
    "        map_scores = []\n",
    "        diversity_scores = []\n",
    "\n",
    "        # Index building time\n",
    "        start_time = time.time()\n",
    "        search_func = ANN_func(embedded_images)\n",
    "        index_build_time = time.time() - start_time\n",
    "\n",
    "        # Perform searches\n",
    "        for indice in pair_indices:\n",
    "            # Run the search\n",
    "            start_time = time.time()\n",
    "            distances, indices = search_func(embedded_images[indice], K)\n",
    "            search_time = time.time() - start_time\n",
    "\n",
    "            search_times.append(search_time)\n",
    "\n",
    "            query_pairs = set([int(indice)+ i for i in range(1,pair_size )])\n",
    "            set_inces = set(indices[:K].tolist()[0])\n",
    "            matches = len(set_inces & query_pairs)\n",
    "\n",
    "            # Calculate various metrics\n",
    "            # Accuracy: Proportion of correct matches in top K results\n",
    "            accuracies.append(matches / K )\n",
    "\n",
    "            # Recall: Proportion of ground truth neighbors found\n",
    "            recalls.append(matches / (pair_size -1))\n",
    "\n",
    "\n",
    "            # Mean Average Precision\n",
    "            precisions = []\n",
    "            for i in range(1, K+1):\n",
    "                precision_at_k = len(set(indices[:i].tolist()[0]) & query_pairs) / i\n",
    "                precisions.append(precision_at_k)\n",
    "            map_scores.append(np.mean(precisions))\n",
    "\n",
    "\n",
    "            # Diversity Score: Measures the variety of retrieved results\n",
    "            # Calculate variance of distances as a simple diversity metric\n",
    "            diversity_scores.append(np.std(distances))\n",
    "\n",
    "        # Compile method-level metrics\n",
    "        method_results = {\n",
    "            'Method Name': method_name,\n",
    "            'Index Build Time': index_build_time,\n",
    "            'Average Search Time': np.mean(search_times),\n",
    "            'Accuracy@K': np.mean(accuracies),\n",
    "            'Recall@K': np.mean(recalls),\n",
    "            'Mean Average Precision@K': np.mean(map_scores),\n",
    "            'Diversity Score': np.mean(diversity_scores)\n",
    "        }\n",
    "\n",
    "        results.append(method_results)\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Optional display\n",
    "    if display:\n",
    "        print(results_df)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0d544be135e91a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "evaluate_ann_search(pair_sample_embd , methods_list , num_searches= 100 , pair_size= numer_of_images_per_class,K= 20 ,display= False )",
   "metadata": {
    "collapsed": false
   },
   "id": "647ac324a0d96738",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
